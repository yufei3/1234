{"ast":null,"code":"var _jsxFileName = \"/Users/apple/Documents/GitHub/VIStA-website/src/pages/Resources.js\";\nimport React from \"react\";\nimport { Helmet } from \"react-helmet\";\nimport Config from \"../Config.json\";\nimport { Splide, SplideSlide } from '@splidejs/react-splide';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst TITLE = \"Resources | \" + Config.SITE_TITLE;\nconst DESC = \"This is the resources page of the VIStA Research Group.\";\nconst CANONICAL = Config.SITE_DOMAIN + \"/resources\";\n\nclass Resources extends React.Component {\n  render() {\n    return /*#__PURE__*/_jsxDEV(\"main\", {\n      children: [/*#__PURE__*/_jsxDEV(Helmet, {\n        children: [/*#__PURE__*/_jsxDEV(\"title\", {\n          children: TITLE\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 17,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"link\", {\n          rel: \"canonical\",\n          href: CANONICAL\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 18,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"meta\", {\n          name: \"description\",\n          content: DESC\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 19,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"meta\", {\n          name: \"theme-color\",\n          content: Config.THEME_COLOR\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 20,\n          columnNumber: 17\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 16,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"h3\", {\n        className: \"resources-h3\",\n        children: \"Featured Talks\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 23,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        id: \"splideArea\",\n        children: /*#__PURE__*/_jsxDEV(Splide, {\n          options: {\n            type: 'loop',\n            gap: '2rem',\n            perPage: 3,\n            fixedHeight: '30rem',\n            focus: 'center',\n            autoWidth: true ///cover: true,\n\n          },\n          \"aria-label\": \"Feature Talks\",\n          children: [/*#__PURE__*/_jsxDEV(SplideSlide, {\n            children: /*#__PURE__*/_jsxDEV(\"div\", {\n              className: \"resources-splide\",\n              id: \"splide-1\",\n              children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n                children: \"Vision and Language: Past, Present, and Future\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 38,\n                columnNumber: 27\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"Computer vision and natural language processing are two key branches of artificial intelligence. Since the goal of computer vision has always been automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images, it is natural for vision and language to come together to enable high-level computer vision tasks. Conversely, information extracted from images and videos can facilitate natural language processing tasks. Recent advances in machine learning and deep learning are facilitating reasoning about images and text in a joint fashion. in this talk, we will review a recently active area of research at the intersection of vision and language, including video-language alignment, image and video captioning, visual question answering, image retrieval using complex text queries, image generation from textual descriptions, language grounding in images and videos, as well as multimodal machine translation and vision-aided grammar induction.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 39,\n                columnNumber: 27\n              }, this)]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 37,\n              columnNumber: 25\n            }, this)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 36,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(SplideSlide, {\n            children: /*#__PURE__*/_jsxDEV(\"div\", {\n              className: \"resources-splide\",\n              id: \"splide-2\",\n              children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n                children: \"COVID-19: What Social Media and Machine Learning Can Inform Us\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 57,\n                columnNumber: 27\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"The COVID-19 pandemic has severely affected people's daily lives and caused tremendous economic losses worldwide. However, its influence on public opinions and people's mental health conditions has not received as much attention. In addition, the related literature in these fields has primarily relied on interviews or surveys, largely limited to small-scale observations. In contrast, the rise of social media provides an opportunity to study many aspects of a pandemic at scale and in real-time. Meanwhile, the recent advances in machine learning and data mining allow us to perform automated data processing and analysis. We will introduce several recent studies ranging from 1) characterizing Twitter users and topics regarding the use of controversial terms for COVID-19, 2) understanding how college students respond differently than the general public to the pandemic, 3) monitoring depression trends throughout COVID-19, to 4) studying consumer hoarding behaviors during the pandemic.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 58,\n                columnNumber: 27\n              }, this)]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 56,\n              columnNumber: 23\n            }, this)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 55,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(SplideSlide, {\n            children: /*#__PURE__*/_jsxDEV(\"div\", {\n              className: \"resources-splide\",\n              id: \"splide-3\",\n              children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n                children: \"Learning with Unpaired Data\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 74,\n                columnNumber: 27\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"Many learning tasks can be summarized as learning a mapping from a structured input to a structured output, such as machine translation, image style transfer, and so on. Such mappings are usually learned on paired training data, where an input sample and its corresponding output are both provided. Collecting paired training data often involves expensive human annotation, and the scale of paired training data is therefore often limited. As a result, the generalization ability of models trained on paired data is also limited. One way to mitigate this issue is learning with unpaired data, which is far less expensive to collect. Taking machine translation as an example, the unpaired training data can be collected separately from newspapers in the source language and target language without any annotation. The challenge of unpaired learning turns into how to align the unpaired data. With carefully designed objectives, unpaired learning has achieved remarkable progress on several tasks. This talk will cover the data collection and training methods of several unpaired learning tasks to illustrate the power of learning with unpaired data.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 75,\n                columnNumber: 27\n              }, this)]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 73,\n              columnNumber: 23\n            }, this)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 72,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(SplideSlide, {\n            children: /*#__PURE__*/_jsxDEV(\"div\", {\n              className: \"resources-splide\",\n              id: \"splide-4\",\n              children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n                children: \"Vision and Language: Past, Present, and Future\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 93,\n                columnNumber: 27\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"Computer vision and natural language processing are two key branches of artificial intelligence. Since the goal of computer vision has always been automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images, it is natural for vision and language to come together to enable high-level computer vision tasks. Conversely, information extracted from images and videos can facilitate natural language processing tasks. Recent advances in machine learning and deep learning are facilitating reasoning about images and text in a joint fashion. in this talk, we will review a recently active area of research at the intersection of vision and language, including video-language alignment, image and video captioning, visual question answering, image retrieval using complex text queries, image generation from textual descriptions, language grounding in images and videos, as well as multimodal machine translation and vision-aided grammar induction.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 94,\n                columnNumber: 27\n              }, this)]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 92,\n              columnNumber: 23\n            }, this)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 91,\n            columnNumber: 21\n          }, this)]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 26,\n          columnNumber: 17\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 25,\n        columnNumber: 13\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 14,\n      columnNumber: 17\n    }, this);\n  }\n\n}\n\nexport default Resources;","map":{"version":3,"sources":["/Users/apple/Documents/GitHub/VIStA-website/src/pages/Resources.js"],"names":["React","Helmet","Config","Splide","SplideSlide","TITLE","SITE_TITLE","DESC","CANONICAL","SITE_DOMAIN","Resources","Component","render","THEME_COLOR","type","gap","perPage","fixedHeight","focus","autoWidth"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,SAAQC,MAAR,QAAqB,cAArB;AACA,OAAOC,MAAP,MAAmB,gBAAnB;AACA,SAASC,MAAT,EAAiBC,WAAjB,QAAoC,wBAApC;;AAEA,MAAMC,KAAK,GAAG,iBAAiBH,MAAM,CAACI,UAAtC;AACA,MAAMC,IAAI,GAAG,yDAAb;AACA,MAAMC,SAAS,GAAGN,MAAM,CAACO,WAAP,GAAqB,YAAvC;;AAGA,MAAMC,SAAN,SAAwBV,KAAK,CAACW,SAA9B,CAAuC;AAEnCC,EAAAA,MAAM,GAAG;AACL,wBAAQ;AAAA,8BAEJ,QAAC,MAAD;AAAA,gCACI;AAAA,oBAAQP;AAAR;AAAA;AAAA;AAAA;AAAA,gBADJ,eAEI;AAAM,UAAA,GAAG,EAAC,WAAV;AAAsB,UAAA,IAAI,EAAEG;AAA5B;AAAA;AAAA;AAAA;AAAA,gBAFJ,eAGI;AAAM,UAAA,IAAI,EAAC,aAAX;AAAyB,UAAA,OAAO,EAAED;AAAlC;AAAA;AAAA;AAAA;AAAA,gBAHJ,eAII;AAAM,UAAA,IAAI,EAAC,aAAX;AAAyB,UAAA,OAAO,EAAEL,MAAM,CAACW;AAAzC;AAAA;AAAA;AAAA;AAAA,gBAJJ;AAAA;AAAA;AAAA;AAAA;AAAA,cAFI,eASJ;AAAI,QAAA,SAAS,EAAC,cAAd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cATI,eAWJ;AAAK,QAAA,EAAE,EAAC,YAAR;AAAA,+BACI,QAAC,MAAD;AAAQ,UAAA,OAAO,EACX;AAACC,YAAAA,IAAI,EAAO,MAAZ;AACAC,YAAAA,GAAG,EAAE,MADL;AAEAC,YAAAA,OAAO,EAAE,CAFT;AAGAC,YAAAA,WAAW,EAAE,OAHb;AAIAC,YAAAA,KAAK,EAAM,QAJX;AAKAC,YAAAA,SAAS,EAAE,IALX,CAMA;;AANA,WADJ;AASU,wBAAW,eATrB;AAAA,kCAUI,QAAC,WAAD;AAAA,mCACI;AAAK,cAAA,SAAS,EAAC,kBAAf;AAAkC,cAAA,EAAE,EAAC,UAArC;AAAA,sCACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBADF,eAEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAFF;AAAA;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,kBAVJ,eA6BI,QAAC,WAAD;AAAA,mCACE;AAAK,cAAA,SAAS,EAAC,kBAAf;AAAkC,cAAA,EAAE,EAAC,UAArC;AAAA,sCACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBADJ,eAEI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAFJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,kBA7BJ,eA8CI,QAAC,WAAD;AAAA,mCACE;AAAK,cAAA,SAAS,EAAC,kBAAf;AAAkC,cAAA,EAAE,EAAC,UAArC;AAAA,sCACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBADJ,eAEI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAFJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,kBA9CJ,eAiEI,QAAC,WAAD;AAAA,mCACE;AAAK,cAAA,SAAS,EAAC,kBAAf;AAAkC,cAAA,EAAE,EAAC,UAArC;AAAA,sCACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBADJ,eAEI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAFJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,kBAjEJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,cAXI;AAAA;AAAA;AAAA;AAAA;AAAA,YAAR;AAoGH;;AAvGkC;;AA0GvC,eAAeT,SAAf","sourcesContent":["import React from \"react\";\nimport {Helmet} from \"react-helmet\";\nimport Config from \"../Config.json\";\nimport { Splide, SplideSlide } from '@splidejs/react-splide';\n\nconst TITLE = \"Resources | \" + Config.SITE_TITLE;\nconst DESC = \"This is the resources page of the VIStA Research Group.\";\nconst CANONICAL = Config.SITE_DOMAIN + \"/resources\";\n\n\nclass Resources extends React.Component{\n    \n    render() {\n        return (<main>\n\n            <Helmet>\n                <title>{TITLE}</title>\n                <link rel=\"canonical\" href={CANONICAL} />\n                <meta name=\"description\" content={DESC}/>\n                <meta name=\"theme-color\" content={Config.THEME_COLOR} />\n            </Helmet>\n\n            <h3 className=\"resources-h3\">Featured Talks</h3>\n\n            <div id=\"splideArea\">\n                <Splide options={\n                    {type     : 'loop',\n                    gap: '2rem',\n                    perPage: 3,\n                    fixedHeight: '30rem',\n                    focus    : 'center',\n                    autoWidth: true,\n                    ///cover: true,\n                    }\n                        } aria-label=\"Feature Talks\">\n                    <SplideSlide>\n                        <div className=\"resources-splide\" id=\"splide-1\">\n                          <h4>Vision and Language: Past, Present, and Future</h4>\n                          <p>Computer vision and natural language processing are two key branches of \n                              artificial intelligence. Since the goal of computer vision has always \n                              been automatic extraction, analysis, and understanding of useful information \n                              from a single image or a sequence of images, it is natural for vision and language \n                              to come together to enable high-level computer vision tasks. Conversely, information \n                              extracted from images and videos can facilitate natural language processing tasks. \n                              Recent advances in machine learning and deep learning are facilitating reasoning \n                              about images and text in a joint fashion. in this talk, we will review a recently \n                              active area of research at the intersection of vision and language, including \n                              video-language alignment, image and video captioning, visual question answering, \n                              image retrieval using complex text queries, image generation from textual descriptions, \n                              language grounding in images and videos, as well as multimodal machine translation and \n                              vision-aided grammar induction.\n                          </p>\n                        </div>\n                    </SplideSlide>\n                    <SplideSlide>\n                      <div className=\"resources-splide\" id=\"splide-2\">\n                          <h4>COVID-19: What Social Media and Machine Learning Can Inform Us</h4>\n                          <p>The COVID-19 pandemic has severely affected people's daily lives and caused tremendous \n                              economic losses worldwide. However, its influence on public opinions and people's mental \n                              health conditions has not received as much attention. In addition, the related literature \n                              in these fields has primarily relied on interviews or surveys, largely limited to small-scale \n                              observations. In contrast, the rise of social media provides an opportunity to study many \n                              aspects of a pandemic at scale and in real-time. Meanwhile, the recent advances in machine \n                              learning and data mining allow us to perform automated data processing and analysis. We will \n                              introduce several recent studies ranging from 1) characterizing Twitter users and topics \n                              regarding the use of controversial terms for COVID-19, 2) understanding how college students \n                              respond differently than the general public to the pandemic, 3) monitoring depression trends \n                              throughout COVID-19, to 4) studying consumer hoarding behaviors during the pandemic.\n                          </p>\n                        </div>\n                    </SplideSlide>\n                    <SplideSlide>\n                      <div className=\"resources-splide\" id=\"splide-3\">\n                          <h4>Learning with Unpaired Data</h4>\n                          <p>Many learning tasks can be summarized as learning a mapping from a structured input to a \n                              structured output, such as machine translation, image style transfer, and so on. \n                              Such mappings are usually learned on paired training data, where an input sample \n                              and its corresponding output are both provided. Collecting paired training data often involves \n                              expensive human annotation, and the scale of paired training data is therefore often limited. \n                              As a result, the generalization ability of models trained on paired data is also limited. \n                              One way to mitigate this issue is learning with unpaired data, which is far less expensive to \n                              collect. Taking machine translation as an example, the unpaired training data can be collected \n                              separately from newspapers in the source language and target language without any annotation. \n                              The challenge of unpaired learning turns into how to align the unpaired data. With carefully \n                              designed objectives, unpaired learning has achieved remarkable progress on several tasks. This \n                              talk will cover the data collection and training methods of several unpaired learning tasks to \n                              illustrate the power of learning with unpaired data.\n                          </p>\n                      </div>\n                    </SplideSlide>\n                    <SplideSlide>\n                      <div className=\"resources-splide\" id=\"splide-4\">\n                          <h4>Vision and Language: Past, Present, and Future</h4>\n                          <p>Computer vision and natural language processing are two key branches of \n                              artificial intelligence. Since the goal of computer vision has always \n                              been automatic extraction, analysis, and understanding of useful information \n                              from a single image or a sequence of images, it is natural for vision and language \n                              to come together to enable high-level computer vision tasks. Conversely, information \n                              extracted from images and videos can facilitate natural language processing tasks. \n                              Recent advances in machine learning and deep learning are facilitating reasoning \n                              about images and text in a joint fashion. in this talk, we will review a recently \n                              active area of research at the intersection of vision and language, including \n                              video-language alignment, image and video captioning, visual question answering, \n                              image retrieval using complex text queries, image generation from textual descriptions, \n                              language grounding in images and videos, as well as multimodal machine translation and \n                              vision-aided grammar induction.\n                          </p>\n                        </div>\n                    </SplideSlide>\n                </Splide>\n            </div>\n\n        </main>);\n    }\n}\n\nexport default Resources;"]},"metadata":{},"sourceType":"module"}
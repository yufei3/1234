{"ast":null,"code":"var _jsxFileName = \"/Users/apple/Documents/GitHub/VIStA-website/src/pages/Resources.js\";\nimport React from \"react\";\nimport { Helmet } from \"react-helmet\";\nimport Config from \"../Config.json\";\nimport { Splide, SplideSlide } from '@splidejs/react-splide';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst TITLE = \"Resources | \" + Config.SITE_TITLE;\nconst DESC = \"This is the resources page of the VIStA Research Group.\";\nconst CANONICAL = Config.SITE_DOMAIN + \"/resources\";\n\nclass Resources extends React.Component {\n  render() {\n    return /*#__PURE__*/_jsxDEV(\"main\", {\n      children: [/*#__PURE__*/_jsxDEV(Helmet, {\n        children: [/*#__PURE__*/_jsxDEV(\"title\", {\n          children: TITLE\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 17,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"link\", {\n          rel: \"canonical\",\n          href: CANONICAL\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 18,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"meta\", {\n          name: \"description\",\n          content: DESC\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 19,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"meta\", {\n          name: \"theme-color\",\n          content: Config.THEME_COLOR\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 20,\n          columnNumber: 17\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 16,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"h3\", {\n        className: \"resources-h3\",\n        children: \"Featured Talks\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 23,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        id: \"splideArea\",\n        children: /*#__PURE__*/_jsxDEV(Splide, {\n          options: {\n            type: 'loop',\n            gap: '2rem',\n            perPage: 3,\n            fixedHeight: '30rem',\n            focus: 'center',\n            autoWidth: true ///cover: true,\n\n          },\n          \"aria-label\": \"Feature Talks\",\n          children: [/*#__PURE__*/_jsxDEV(SplideSlide, {\n            children: /*#__PURE__*/_jsxDEV(\"div\", {\n              className: \"resources-splide\",\n              id: \"splide-1\",\n              children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n                children: \"Vision and Language: Past, Present, and Future\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 38,\n                columnNumber: 27\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"Computer vision and natural language processing are two key branches of artificial intelligence. Since the goal of computer vision has always been automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images, it is natural for vision and language to come together to enable high-level computer vision tasks. Conversely, information extracted from images and videos can facilitate natural language processing tasks. Recent advances in machine learning and deep learning are facilitating reasoning about images and text in a joint fashion. in this talk, we will review a recently active area of research at the intersection of vision and language, including video-language alignment, image and video captioning, visual question answering, image retrieval using complex text queries, image generation from textual descriptions, language grounding in images and videos, as well as multimodal machine translation and vision-aided grammar induction.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 39,\n                columnNumber: 27\n              }, this)]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 37,\n              columnNumber: 25\n            }, this)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 36,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(SplideSlide, {\n            children: /*#__PURE__*/_jsxDEV(\"div\", {\n              className: \"resources-splide\",\n              id: \"splide-2\",\n              children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n                children: \"COVID-19: What Social Media and Machine Learning Can Inform Us\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 57,\n                columnNumber: 27\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"The COVID-19 pandemic has severely affected people's daily lives and caused tremendous economic losses worldwide. However, its influence on public opinions and people's mental health conditions has not received as much attention. In addition, the related literature in these fields has primarily relied on interviews or surveys, largely limited to small-scale observations. In contrast, the rise of social media provides an opportunity to study many aspects of a pandemic at scale and in real-time. Meanwhile, the recent advances in machine learning and data mining allow us to perform automated data processing and analysis. We will introduce several recent studies ranging from 1) characterizing Twitter users and topics regarding the use of controversial terms for COVID-19, 2) understanding how college students respond differently than the general public to the pandemic, 3) monitoring depression trends throughout COVID-19, to 4) studying consumer hoarding behaviors during the pandemic.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 58,\n                columnNumber: 27\n              }, this)]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 56,\n              columnNumber: 23\n            }, this)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 55,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(SplideSlide, {\n            children: /*#__PURE__*/_jsxDEV(\"div\", {\n              className: \"resources-splide\",\n              id: \"splide-3\",\n              children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n                children: \"Big Data Better Life\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 74,\n                columnNumber: 27\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"We live in the age of big data. The biggest big data is big visual data, which includes images and other associated information. The biggest challenge is to develop effective computational methods for making sense of such massive visual data. Unlike text which is clean, segmented, compact, one dimensional and indexable, visual content is noisy, unsegmented, high entropy and multidimensional. In this talk, we present a few recent advances towards the ultimate goal of using big data, in particular large-scale and rich multi-modality data, to achieve robust intelligence in order to understand and improve life in terms of healthcare, well-being, politics, business, infotainment, and so on.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 75,\n                columnNumber: 27\n              }, this)]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 73,\n              columnNumber: 23\n            }, this)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 72,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(SplideSlide, {\n            children: /*#__PURE__*/_jsxDEV(\"div\", {\n              className: \"resources-splide\",\n              id: \"splide-4\",\n              children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n                children: \"Vision and Language: Past, Present, and Future\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 89,\n                columnNumber: 27\n              }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n                children: \"Computer vision and natural language processing are two key branches of artificial intelligence. Since the goal of computer vision has always been automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images, it is natural for vision and language to come together to enable high-level computer vision tasks. Conversely, information extracted from images and videos can facilitate natural language processing tasks. Recent advances in machine learning and deep learning are facilitating reasoning about images and text in a joint fashion. in this talk, we will review a recently active area of research at the intersection of vision and language, including video-language alignment, image and video captioning, visual question answering, image retrieval using complex text queries, image generation from textual descriptions, language grounding in images and videos, as well as multimodal machine translation and vision-aided grammar induction.\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 90,\n                columnNumber: 27\n              }, this)]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 88,\n              columnNumber: 23\n            }, this)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 87,\n            columnNumber: 21\n          }, this)]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 26,\n          columnNumber: 17\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 25,\n        columnNumber: 13\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 14,\n      columnNumber: 17\n    }, this);\n  }\n\n}\n\nexport default Resources;","map":{"version":3,"sources":["/Users/apple/Documents/GitHub/VIStA-website/src/pages/Resources.js"],"names":["React","Helmet","Config","Splide","SplideSlide","TITLE","SITE_TITLE","DESC","CANONICAL","SITE_DOMAIN","Resources","Component","render","THEME_COLOR","type","gap","perPage","fixedHeight","focus","autoWidth"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,SAAQC,MAAR,QAAqB,cAArB;AACA,OAAOC,MAAP,MAAmB,gBAAnB;AACA,SAASC,MAAT,EAAiBC,WAAjB,QAAoC,wBAApC;;AAEA,MAAMC,KAAK,GAAG,iBAAiBH,MAAM,CAACI,UAAtC;AACA,MAAMC,IAAI,GAAG,yDAAb;AACA,MAAMC,SAAS,GAAGN,MAAM,CAACO,WAAP,GAAqB,YAAvC;;AAGA,MAAMC,SAAN,SAAwBV,KAAK,CAACW,SAA9B,CAAuC;AAEnCC,EAAAA,MAAM,GAAG;AACL,wBAAQ;AAAA,8BAEJ,QAAC,MAAD;AAAA,gCACI;AAAA,oBAAQP;AAAR;AAAA;AAAA;AAAA;AAAA,gBADJ,eAEI;AAAM,UAAA,GAAG,EAAC,WAAV;AAAsB,UAAA,IAAI,EAAEG;AAA5B;AAAA;AAAA;AAAA;AAAA,gBAFJ,eAGI;AAAM,UAAA,IAAI,EAAC,aAAX;AAAyB,UAAA,OAAO,EAAED;AAAlC;AAAA;AAAA;AAAA;AAAA,gBAHJ,eAII;AAAM,UAAA,IAAI,EAAC,aAAX;AAAyB,UAAA,OAAO,EAAEL,MAAM,CAACW;AAAzC;AAAA;AAAA;AAAA;AAAA,gBAJJ;AAAA;AAAA;AAAA;AAAA;AAAA,cAFI,eASJ;AAAI,QAAA,SAAS,EAAC,cAAd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cATI,eAWJ;AAAK,QAAA,EAAE,EAAC,YAAR;AAAA,+BACI,QAAC,MAAD;AAAQ,UAAA,OAAO,EACX;AAACC,YAAAA,IAAI,EAAO,MAAZ;AACAC,YAAAA,GAAG,EAAE,MADL;AAEAC,YAAAA,OAAO,EAAE,CAFT;AAGAC,YAAAA,WAAW,EAAE,OAHb;AAIAC,YAAAA,KAAK,EAAM,QAJX;AAKAC,YAAAA,SAAS,EAAE,IALX,CAMA;;AANA,WADJ;AASU,wBAAW,eATrB;AAAA,kCAUI,QAAC,WAAD;AAAA,mCACI;AAAK,cAAA,SAAS,EAAC,kBAAf;AAAkC,cAAA,EAAE,EAAC,UAArC;AAAA,sCACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBADF,eAEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAFF;AAAA;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,kBAVJ,eA6BI,QAAC,WAAD;AAAA,mCACE;AAAK,cAAA,SAAS,EAAC,kBAAf;AAAkC,cAAA,EAAE,EAAC,UAArC;AAAA,sCACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBADJ,eAEI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAFJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,kBA7BJ,eA8CI,QAAC,WAAD;AAAA,mCACE;AAAK,cAAA,SAAS,EAAC,kBAAf;AAAkC,cAAA,EAAE,EAAC,UAArC;AAAA,sCACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBADJ,eAEI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAFJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,kBA9CJ,eA6DI,QAAC,WAAD;AAAA,mCACE;AAAK,cAAA,SAAS,EAAC,kBAAf;AAAkC,cAAA,EAAE,EAAC,UAArC;AAAA,sCACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBADJ,eAEI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAFJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,kBA7DJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,cAXI;AAAA;AAAA;AAAA;AAAA;AAAA,YAAR;AAgGH;;AAnGkC;;AAsGvC,eAAeT,SAAf","sourcesContent":["import React from \"react\";\nimport {Helmet} from \"react-helmet\";\nimport Config from \"../Config.json\";\nimport { Splide, SplideSlide } from '@splidejs/react-splide';\n\nconst TITLE = \"Resources | \" + Config.SITE_TITLE;\nconst DESC = \"This is the resources page of the VIStA Research Group.\";\nconst CANONICAL = Config.SITE_DOMAIN + \"/resources\";\n\n\nclass Resources extends React.Component{\n    \n    render() {\n        return (<main>\n\n            <Helmet>\n                <title>{TITLE}</title>\n                <link rel=\"canonical\" href={CANONICAL} />\n                <meta name=\"description\" content={DESC}/>\n                <meta name=\"theme-color\" content={Config.THEME_COLOR} />\n            </Helmet>\n\n            <h3 className=\"resources-h3\">Featured Talks</h3>\n\n            <div id=\"splideArea\">\n                <Splide options={\n                    {type     : 'loop',\n                    gap: '2rem',\n                    perPage: 3,\n                    fixedHeight: '30rem',\n                    focus    : 'center',\n                    autoWidth: true,\n                    ///cover: true,\n                    }\n                        } aria-label=\"Feature Talks\">\n                    <SplideSlide>\n                        <div className=\"resources-splide\" id=\"splide-1\">\n                          <h4>Vision and Language: Past, Present, and Future</h4>\n                          <p>Computer vision and natural language processing are two key branches of \n                              artificial intelligence. Since the goal of computer vision has always \n                              been automatic extraction, analysis, and understanding of useful information \n                              from a single image or a sequence of images, it is natural for vision and language \n                              to come together to enable high-level computer vision tasks. Conversely, information \n                              extracted from images and videos can facilitate natural language processing tasks. \n                              Recent advances in machine learning and deep learning are facilitating reasoning \n                              about images and text in a joint fashion. in this talk, we will review a recently \n                              active area of research at the intersection of vision and language, including \n                              video-language alignment, image and video captioning, visual question answering, \n                              image retrieval using complex text queries, image generation from textual descriptions, \n                              language grounding in images and videos, as well as multimodal machine translation and \n                              vision-aided grammar induction.\n                          </p>\n                        </div>\n                    </SplideSlide>\n                    <SplideSlide>\n                      <div className=\"resources-splide\" id=\"splide-2\">\n                          <h4>COVID-19: What Social Media and Machine Learning Can Inform Us</h4>\n                          <p>The COVID-19 pandemic has severely affected people's daily lives and caused tremendous \n                              economic losses worldwide. However, its influence on public opinions and people's mental \n                              health conditions has not received as much attention. In addition, the related literature \n                              in these fields has primarily relied on interviews or surveys, largely limited to small-scale \n                              observations. In contrast, the rise of social media provides an opportunity to study many \n                              aspects of a pandemic at scale and in real-time. Meanwhile, the recent advances in machine \n                              learning and data mining allow us to perform automated data processing and analysis. We will \n                              introduce several recent studies ranging from 1) characterizing Twitter users and topics \n                              regarding the use of controversial terms for COVID-19, 2) understanding how college students \n                              respond differently than the general public to the pandemic, 3) monitoring depression trends \n                              throughout COVID-19, to 4) studying consumer hoarding behaviors during the pandemic.\n                          </p>\n                        </div>\n                    </SplideSlide>\n                    <SplideSlide>\n                      <div className=\"resources-splide\" id=\"splide-3\">\n                          <h4>Big Data Better Life</h4>\n                          <p>We live in the age of big data. The biggest big data is big visual data, \n                              which includes images and other associated information. The biggest challenge \n                              is to develop effective computational methods for making sense of such massive \n                              visual data. Unlike text which is clean, segmented, compact, one dimensional \n                              and indexable, visual content is noisy, unsegmented, high entropy and multidimensional. \n                              In this talk, we present a few recent advances towards the ultimate goal of using big data, \n                              in particular large-scale and rich multi-modality data, to achieve robust intelligence in \n                              order to understand and improve life in terms of healthcare, well-being, politics, business, \n                              infotainment, and so on.\n                          </p>\n                      </div>\n                    </SplideSlide>\n                    <SplideSlide>\n                      <div className=\"resources-splide\" id=\"splide-4\">\n                          <h4>Vision and Language: Past, Present, and Future</h4>\n                          <p>Computer vision and natural language processing are two key branches of \n                              artificial intelligence. Since the goal of computer vision has always \n                              been automatic extraction, analysis, and understanding of useful information \n                              from a single image or a sequence of images, it is natural for vision and language \n                              to come together to enable high-level computer vision tasks. Conversely, information \n                              extracted from images and videos can facilitate natural language processing tasks. \n                              Recent advances in machine learning and deep learning are facilitating reasoning \n                              about images and text in a joint fashion. in this talk, we will review a recently \n                              active area of research at the intersection of vision and language, including \n                              video-language alignment, image and video captioning, visual question answering, \n                              image retrieval using complex text queries, image generation from textual descriptions, \n                              language grounding in images and videos, as well as multimodal machine translation and \n                              vision-aided grammar induction.\n                          </p>\n                        </div>\n                    </SplideSlide>\n                </Splide>\n            </div>\n\n        </main>);\n    }\n}\n\nexport default Resources;"]},"metadata":{},"sourceType":"module"}